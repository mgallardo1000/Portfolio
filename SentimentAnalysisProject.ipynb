{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentimental Analysis \n",
    "This Jupyter notebook demonstrates the sentimental analysis of Amazon Inc reviews. The notebook is divided into several parts:\n",
    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Fetching and Preprocessing Amazon Inc reviews\n",
    "\n",
    "In this part, I gathered the dataset from Kaggle.com which consists of a training dataset and a test dataset and preprocess the data for further inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__2 Great CD: My lovely Pat has one of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__2 One of the best game music soundtra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__1 Batteries died within a year ...: I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__2 works fine, but Maha Energy is bett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__2 Great for the non-audiophile: Revie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  __label__2 Great CD: My lovely Pat has one of ...\n",
       "1  __label__2 One of the best game music soundtra...\n",
       "2  __label__1 Batteries died within a year ...: I...\n",
       "3  __label__2 works fine, but Maha Energy is bett...\n",
       "4  __label__2 Great for the non-audiophile: Revie..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "# Ensure the file path is correct\n",
    "file_path = '/Users/tobe2.0/Documents/test.ft.txt'\n",
    "\n",
    "# Load the data, assuming the file is tab-separated and has no headers\n",
    "try:\n",
    "    test_data = pd.read_csv(file_path, delimiter='\\t', header=None, names=['Text'], nrows = 1000)\n",
    "    display(test_data.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found at {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__2 Stuning even for the non-gamer: Thi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__2 The best soundtrack ever to anythin...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__2 Amazing!: This soundtrack is my fav...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__2 Excellent Soundtrack: I truly like ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__2 Remember, Pull Your Jaw Off The Flo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Sentiment\n",
       "0  __label__2 Stuning even for the non-gamer: Thi...        NaN\n",
       "1  __label__2 The best soundtrack ever to anythin...        NaN\n",
       "2  __label__2 Amazing!: This soundtrack is my fav...        NaN\n",
       "3  __label__2 Excellent Soundtrack: I truly like ...        NaN\n",
       "4  __label__2 Remember, Pull Your Jaw Off The Flo...        NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "# Ensure the file path is correct\n",
    "file_path = '/Users/tobe2.0/Documents/train.ft.txt'\n",
    "\n",
    "# Load the data, assuming the file is tab-separated and has no headers\n",
    "try:\n",
    "    train_data = pd.read_csv(file_path, delimiter='\\t', header=None, names=['Text','Sentiment'], nrows = 1000)\n",
    "    display(train_data.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found at {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning train_data:\n",
      "                                                 Text  \\\n",
      "0  __label__2 Stuning even for the non-gamer: Thi...   \n",
      "1  __label__2 The best soundtrack ever to anythin...   \n",
      "2  __label__2 Amazing!: This soundtrack is my fav...   \n",
      "3  __label__2 Excellent Soundtrack: I truly like ...   \n",
      "4  __label__2 Remember, Pull Your Jaw Off The Flo...   \n",
      "\n",
      "                                        Cleaned_Text  \n",
      "0  __label__2 Stuning even for the non-gamer: Thi...  \n",
      "1  __label__2 The best soundtrack ever to anythin...  \n",
      "2  __label__2 Amazing!: This soundtrack is my fav...  \n",
      "3  __label__2 Excellent Soundtrack: I truly like ...  \n",
      "4  __label__2 Remember, Pull Your Jaw Off The Flo...  \n",
      "After cleaning test_data:\n",
      "                                                 Text  \\\n",
      "0  __label__2 Great CD: My lovely Pat has one of ...   \n",
      "1  __label__2 One of the best game music soundtra...   \n",
      "2  __label__1 Batteries died within a year ...: I...   \n",
      "3  __label__2 works fine, but Maha Energy is bett...   \n",
      "4  __label__2 Great for the non-audiophile: Revie...   \n",
      "\n",
      "                                        Cleaned_Text  \n",
      "0  __label__2 Great CD: My lovely Pat has one of ...  \n",
      "1  __label__2 One of the best game music soundtra...  \n",
      "2  __label__1 Batteries died within a year ...: I...  \n",
      "3  __label__2 works fine, but Maha Energy is bett...  \n",
      "4  __label__2 Great for the non-audiophile: Revie...  \n"
     ]
    }
   ],
   "source": [
    "import html\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    # Remove HTML entities\n",
    "    text = html.unescape(text)\n",
    "    # Remove hyperlinks\n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '', text)\n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the 'Text' column in train_data and test_data\n",
    "train_data['Cleaned_Text'] = train_data['Text'].apply(clean_text)\n",
    "test_data['Cleaned_Text'] = test_data['Text'].apply(clean_text)\n",
    "\n",
    "# Display the cleaned data\n",
    "print(\"After cleaning train_data:\\n\", train_data[['Text', 'Cleaned_Text']].head())\n",
    "print(\"After cleaning test_data:\\n\", test_data[['Text', 'Cleaned_Text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After replacing slang in train_data:\n",
      "                                         Cleaned_Text  \\\n",
      "0  __label__2 Stuning even for the non-gamer: Thi...   \n",
      "1  __label__2 The best soundtrack ever to anythin...   \n",
      "2  __label__2 Amazing!: This soundtrack is my fav...   \n",
      "3  __label__2 Excellent Soundtrack: I truly like ...   \n",
      "4  __label__2 Remember, Pull Your Jaw Off The Flo...   \n",
      "\n",
      "                                 With_Slang_Replaced  \n",
      "0  __label__2 Stuning even for the non-gamer: Thi...  \n",
      "1  __label__2 The best soundtrack ever to anythin...  \n",
      "2  __label__2 Amazing!: This soundtrack is my fav...  \n",
      "3  __label__2 Excellent Soundtrack: I truly like ...  \n",
      "4  __label__2 Remember, Pull Your Jaw Off The Flo...  \n",
      "After replacing slang in test_data:\n",
      "                                         Cleaned_Text  \\\n",
      "0  __label__2 Great CD: My lovely Pat has one of ...   \n",
      "1  __label__2 One of the best game music soundtra...   \n",
      "2  __label__1 Batteries died within a year ...: I...   \n",
      "3  __label__2 works fine, but Maha Energy is bett...   \n",
      "4  __label__2 Great for the non-audiophile: Revie...   \n",
      "\n",
      "                                 With_Slang_Replaced  \n",
      "0  __label__2 Great CD: My lovely Pat has one of ...  \n",
      "1  __label__2 One of the best game music soundtra...  \n",
      "2  __label__1 Batteries died within a year ...: I...  \n",
      "3  __label__2 works fine, but Maha Energy is bett...  \n",
      "4  __label__2 Great for the non-audiophile: Revie...  \n"
     ]
    }
   ],
   "source": [
    "# Load slang dictionary from \"slang.txt\"\n",
    "with open(\"slang.txt\", 'r', encoding='utf-8') as slang_file:\n",
    "    slang_content = slang_file.read().split('\\n')\n",
    "\n",
    "# Create slang word and meaning lists\n",
    "slang_word = []\n",
    "meaning = []\n",
    "for line in slang_content:\n",
    "    temp = line.split(\"=\")\n",
    "    if len(temp) == 2:  # Ensure there are exactly two parts (word and meaning)\n",
    "        slang_word.append(temp[0].strip())\n",
    "        meaning.append(temp[1].strip())\n",
    "\n",
    "# Function to replace slang words with their meanings\n",
    "def replace_slang(text):\n",
    "    text_tokens = text.split()\n",
    "    for i, word in enumerate(text_tokens):\n",
    "        if word in slang_word:\n",
    "            idx = slang_word.index(word)\n",
    "            text_tokens[i] = meaning[idx]\n",
    "    return \" \".join(text_tokens)\n",
    "\n",
    "# Apply the slang replacement function to the 'Text' column\n",
    "train_data['With_Slang_Replaced'] = train_data['Cleaned_Text'].apply(replace_slang)\n",
    "test_data['With_Slang_Replaced'] = test_data['Cleaned_Text'].apply(replace_slang)\n",
    "\n",
    "# Display the data after slang replacement\n",
    "print(\"After replacing slang in train_data:\\n\", train_data[['Cleaned_Text', 'With_Slang_Replaced']].head())\n",
    "print(\"After replacing slang in test_data:\\n\", test_data[['Cleaned_Text', 'With_Slang_Replaced']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing stopwords from train_data:\n",
      "                                  With_Slang_Replaced  \\\n",
      "0  __label__2 Stuning even for the non-gamer: Thi...   \n",
      "1  __label__2 The best soundtrack ever to anythin...   \n",
      "2  __label__2 Amazing!: This soundtrack is my fav...   \n",
      "3  __label__2 Excellent Soundtrack: I truly like ...   \n",
      "4  __label__2 Remember, Pull Your Jaw Off The Flo...   \n",
      "\n",
      "                                   Without_Stopwords  \n",
      "0  __label__2 Stuning even non-gamer: This sound ...  \n",
      "1  __label__2 The best soundtrack ever anything.:...  \n",
      "2  __label__2 Amazing!: This soundtrack favorite ...  \n",
      "3  __label__2 Excellent Soundtrack: I truly like ...  \n",
      "4  __label__2 Remember, Pull Your Jaw Off The Flo...  \n",
      "After removing stopwords from test_data:\n",
      "                                  With_Slang_Replaced  \\\n",
      "0  __label__2 Great CD: My lovely Pat has one of ...   \n",
      "1  __label__2 One of the best game music soundtra...   \n",
      "2  __label__1 Batteries died within a year ...: I...   \n",
      "3  __label__2 works fine, but Maha Energy is bett...   \n",
      "4  __label__2 Great for the non-audiophile: Revie...   \n",
      "\n",
      "                                   Without_Stopwords  \n",
      "0  __label__2 Great CD: My lovely Pat one GREAT v...  \n",
      "1  __label__2 One best game music soundtracks - g...  \n",
      "2  __label__1 Batteries died within year ...: I b...  \n",
      "3  __label__2 works fine, Maha Energy better: Che...  \n",
      "4  __label__2 Great non-audiophile: Reviewed quit...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load stopwords from file\n",
    "with open(\"stopwords.txt\", 'r', encoding='utf-8') as stopwords_file:\n",
    "    stopwords = stopwords_file.read().split()\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    text_tokens = text.split()\n",
    "    clean_text = [word for word in text_tokens if word not in stopwords]\n",
    "    return \" \".join(clean_text)\n",
    "\n",
    "# Apply the stopwords removal function to the 'Text' column\n",
    "train_data['Without_Stopwords'] = train_data['With_Slang_Replaced'].apply(remove_stopwords)\n",
    "test_data['Without_Stopwords'] = test_data['With_Slang_Replaced'].apply(remove_stopwords)\n",
    "\n",
    "# Display the data after stopwords removal\n",
    "print(\"After removing stopwords from train_data:\\n\", train_data[['With_Slang_Replaced', 'Without_Stopwords']].head())\n",
    "print(\"After removing stopwords from test_data:\\n\", test_data[['With_Slang_Replaced', 'Without_Stopwords']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Label Distribution Before Separation:\n",
      "__label__1    538\n",
      "__label__2    462\n",
      "Name: Label_From_Text, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the distribution of labels in 'Processed_Text' before separation\n",
    "train_data['Label_From_Text'] = train_data['With_Slang_Replaced'].apply(lambda x: x.split()[0])  # Extract label\n",
    "print(\"Train Data Label Distribution Before Separation:\")\n",
    "print(train_data['Label_From_Text'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After processing train_data:\n",
      "                                    Without_Stopwords  \\\n",
      "0  __label__2 Stuning even non-gamer: This sound ...   \n",
      "1  __label__2 The best soundtrack ever anything.:...   \n",
      "2  __label__2 Amazing!: This soundtrack favorite ...   \n",
      "3  __label__2 Excellent Soundtrack: I truly like ...   \n",
      "4  __label__2 Remember, Pull Your Jaw Off The Flo...   \n",
      "\n",
      "                                      Processed_Text  \n",
      "0  __label__2  stuning even non-gamer:  this soun...  \n",
      "1  __label__2  the best soundtrack ever anything....  \n",
      "2  __label__2  amazing!:  this soundtrack favorit...  \n",
      "3  __label__2  excellent  soundtrack:  i truly li...  \n",
      "4  __label__2  remember,  pull  your  jaw  off  t...  \n",
      "After processing test_data:\n",
      "                                    Without_Stopwords  \\\n",
      "0  __label__2 Great CD: My lovely Pat one GREAT v...   \n",
      "1  __label__2 One best game music soundtracks - g...   \n",
      "2  __label__1 Batteries died within year ...: I b...   \n",
      "3  __label__2 works fine, Maha Energy better: Che...   \n",
      "4  __label__2 Great non-audiophile: Reviewed quit...   \n",
      "\n",
      "                                      Processed_Text  \n",
      "0  __label__2  great  cd:  my lovely  pat one  gr...  \n",
      "1  __label__2  one best game music soundtracks - ...  \n",
      "2  __label__1  batteries died within year ...:  i...  \n",
      "3  __label__2 works fine,  maha  energy better:  ...  \n",
      "4  __label__2  great non-audiophile:  reviewed qu...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Dictionary consisting of contractions and their expanded values\n",
    "Apos_dict = {\"'s\": \" is\", \"n't\": \" not\", \"'m\": \" am\", \"'ll\": \" will\",\n",
    "             \"'d\": \" would\", \"'ve\": \" have\", \"'re\": \" are\"}\n",
    "\n",
    "# Function to replace contractions and process text\n",
    "def replace_contractions(text):\n",
    "    # Replace contractions\n",
    "    for key, value in Apos_dict.items():\n",
    "        text = text.replace(key, value)\n",
    "    \n",
    "    # Separate words stuck together (if needed)\n",
    "    text = \" \".join([s for s in re.split(\"([A-Z][a-z]+[^A-Z]*)\", text) if s])\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the replacement function to the 'Text' column in train_data and test_data\n",
    "train_data['Processed_Text'] = train_data['Without_Stopwords'].apply(replace_contractions)\n",
    "test_data['Processed_Text'] = test_data['Without_Stopwords'].apply(replace_contractions)\n",
    "\n",
    "# Display the processed data\n",
    "print(\"After processing train_data:\\n\", train_data[['Without_Stopwords', 'Processed_Text']].head())\n",
    "print(\"After processing test_data:\\n\", test_data[['Without_Stopwords', 'Processed_Text']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Label Distribution Before Separation:\n",
      "__label__1    538\n",
      "__label__2    462\n",
      "Name: Label_From_Text, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the distribution of labels in 'Processed_Text' before separation\n",
    "train_data['Label_From_Text'] = train_data['Processed_Text'].apply(lambda x: x.split()[0])  # Extract label\n",
    "print(\"Train Data Label Distribution Before Separation:\")\n",
    "print(train_data['Label_From_Text'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After final cleaning train_data:\n",
      "                                       Processed_Text  \\\n",
      "0  __label__2  stuning even non-gamer:  this soun...   \n",
      "1  __label__2  the best soundtrack ever anything....   \n",
      "2  __label__2  amazing!:  this soundtrack favorit...   \n",
      "3  __label__2  excellent  soundtrack:  i truly li...   \n",
      "4  __label__2  remember,  pull  your  jaw  off  t...   \n",
      "\n",
      "                                        Cleaned_Text  \n",
      "0  label2 tuning even nongamer this sound track b...  \n",
      "1  label2 the best soundtrack ever anything i am ...  \n",
      "2  label2 amazing this soundtrack favorite music ...  \n",
      "3  label2 excellent soundtrack i truly like sound...  \n",
      "4  label2 remember pull your jaw off the floor af...  \n",
      "After final cleaning test_data:\n",
      "                                       Processed_Text  \\\n",
      "0  __label__2  great  cd:  my lovely  pat one  gr...   \n",
      "1  __label__2  one best game music soundtracks - ...   \n",
      "2  __label__1  batteries died within year ...:  i...   \n",
      "3  __label__2 works fine,  maha  energy better:  ...   \n",
      "4  __label__2  great non-audiophile:  reviewed qu...   \n",
      "\n",
      "                                        Cleaned_Text  \n",
      "0  label2 great cd my lovely pat one great voices...  \n",
      "1  label2 one best game music soundtracks game i ...  \n",
      "2  label1 batteries died within year  i bought ch...  \n",
      "3  label2 works fine mara energy better check mar...  \n",
      "4  label2 great nonaudiophile reviewed quite bit ...  \n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import string\n",
    "from autocorrect import Speller\n",
    "\n",
    "# Standardize repeating letters (one letter should not appear more than twice in a row)\n",
    "def standardize_repeating_letters(text):\n",
    "    return ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n",
    "\n",
    "# Autocorrect spelling mistakes\n",
    "def autocorrect_spelling(text):\n",
    "    spell = Speller(lang='en')\n",
    "    return spell(text)\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text_final(text):\n",
    "    # Standardize repeating letters\n",
    "    text = standardize_repeating_letters(text)\n",
    "\n",
    "    # Autocorrect spelling mistakes\n",
    "    text = autocorrect_spelling(text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    clean_text = [word for word in text.split() if word not in string.punctuation]\n",
    "    text = \" \".join(clean_text)\n",
    "\n",
    "    # Remove non-ASCII characters\n",
    "    text = ''.join([char for char in text if ord(char) < 128])\n",
    "\n",
    "    # Remove non-alphanumeric characters (keeping spaces)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply the final cleaning function to the 'Processed_Text' column\n",
    "train_data['Cleaned_Text'] = train_data['Processed_Text'].apply(clean_text_final)\n",
    "test_data['Cleaned_Text'] = test_data['Processed_Text'].apply(clean_text_final)\n",
    "\n",
    "# Display the cleaned data\n",
    "print(\"After final cleaning train_data:\\n\", train_data[['Processed_Text', 'Cleaned_Text']].head())\n",
    "print(\"After final cleaning test_data:\\n\", test_data[['Processed_Text', 'Cleaned_Text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "                                         Review_Text  Label\n",
      "0  tuning even nongamer this sound track beautifu...      0\n",
      "1  the best soundtrack ever anything i am reading...      0\n",
      "2  amazing this soundtrack favorite music time ha...      0\n",
      "3  excellent soundtrack i truly like soundtrack i...      0\n",
      "4  remember pull your jaw off the floor after hea...      0\n",
      "\n",
      "Test Data:\n",
      "                                         Review_Text  Label\n",
      "0  great cd my lovely pat one great voices genera...      0\n",
      "1  one best game music soundtracks game i did not...      0\n",
      "2  batteries died within year  i bought charger j...      1\n",
      "3  works fine mara energy better check mara energ...      0\n",
      "4  great nonaudiophile reviewed quite bit combo p...      0\n"
     ]
    }
   ],
   "source": [
    "# Define a function to split the label and review\n",
    "def split_review_and_label(text):\n",
    "    if text.startswith('label2'):\n",
    "        return text[7:], 'label2'  # Extract review (after 'label2 ') and return label\n",
    "    elif text.startswith('label1'):\n",
    "        return text[7:], 'label1'  # Extract review (after 'label1 ') and return label\n",
    "    else:\n",
    "        return text, None  # Return original text and None if label is missing\n",
    "\n",
    "# Apply the function to train_data\n",
    "train_data[['Review_Text', 'Label']] = train_data['Cleaned_Text'].apply(lambda x: pd.Series(split_review_and_label(x)))\n",
    "\n",
    "# Apply the function to test_data\n",
    "test_data[['Review_Text', 'Label']] = test_data['Cleaned_Text'].apply(lambda x: pd.Series(split_review_and_label(x)))\n",
    "\n",
    "# Drop rows where Label is None (if any)\n",
    "train_data = train_data[train_data['Label'].notna()]\n",
    "test_data = test_data[test_data['Label'].notna()]\n",
    "\n",
    "# Map label1 and label2 to numerical values\n",
    "label_mapping = {'label1': 1, 'label2': 0}\n",
    "train_data['Label'] = train_data['Label'].map(label_mapping)\n",
    "test_data['Label'] = test_data['Label'].map(label_mapping)\n",
    "\n",
    "# Display the updated DataFrames\n",
    "print(\"Train Data:\")\n",
    "print(train_data[['Review_Text', 'Label']].head())\n",
    "\n",
    "print(\"\\nTest Data:\")\n",
    "print(test_data[['Review_Text', 'Label']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Train Data Label Distribution:\n",
      "1    538\n",
      "0    462\n",
      "Name: Label, dtype: int64\n",
      "Test Data Label Distribution:\n",
      "0    502\n",
      "1    498\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Full Train Data Label Distribution:\")\n",
    "print(train_data['Label'].value_counts())\n",
    "\n",
    "print(\"Test Data Label Distribution:\")\n",
    "print(test_data['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF VECTORIZATION AND WORD2VEC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Features for train data:\n",
      "    0070412901  0072316373   07   10  100  100th  101  10102  10103  1077  ...  \\\n",
      "0         0.0         0.0  0.0  0.0  0.0    0.0  0.0    0.0    0.0   0.0  ...   \n",
      "1         0.0         0.0  0.0  0.0  0.0    0.0  0.0    0.0    0.0   0.0  ...   \n",
      "2         0.0         0.0  0.0  0.0  0.0    0.0  0.0    0.0    0.0   0.0  ...   \n",
      "3         0.0         0.0  0.0  0.0  0.0    0.0  0.0    0.0    0.0   0.0  ...   \n",
      "4         0.0         0.0  0.0  0.0  0.0    0.0  0.0    0.0    0.0   0.0  ...   \n",
      "\n",
      "   zapruder  zebra   zelbess  zen  zero  zippered  zone  zoned  zoo   zz  \n",
      "0       0.0    0.0  0.000000  0.0   0.0       0.0   0.0    0.0  0.0  0.0  \n",
      "1       0.0    0.0  0.000000  0.0   0.0       0.0   0.0    0.0  0.0  0.0  \n",
      "2       0.0    0.0  0.000000  0.0   0.0       0.0   0.0    0.0  0.0  0.0  \n",
      "3       0.0    0.0  0.119852  0.0   0.0       0.0   0.0    0.0  0.0  0.0  \n",
      "4       0.0    0.0  0.000000  0.0   0.0       0.0   0.0    0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 7864 columns]\n",
      "TF-IDF Features for test data:\n",
      "    0070412901  0072316373   07   10       100  100th  101  10102  10103  1077  \\\n",
      "0         0.0         0.0  0.0  0.0  0.000000    0.0  0.0    0.0    0.0   0.0   \n",
      "1         0.0         0.0  0.0  0.0  0.000000    0.0  0.0    0.0    0.0   0.0   \n",
      "2         0.0         0.0  0.0  0.0  0.000000    0.0  0.0    0.0    0.0   0.0   \n",
      "3         0.0         0.0  0.0  0.0  0.164456    0.0  0.0    0.0    0.0   0.0   \n",
      "4         0.0         0.0  0.0  0.0  0.000000    0.0  0.0    0.0    0.0   0.0   \n",
      "\n",
      "   ...  zapruder  zebra  zelbess  zen  zero  zippered  zone  zoned  zoo   zz  \n",
      "0  ...       0.0    0.0      0.0  0.0   0.0       0.0   0.0    0.0  0.0  0.0  \n",
      "1  ...       0.0    0.0      0.0  0.0   0.0       0.0   0.0    0.0  0.0  0.0  \n",
      "2  ...       0.0    0.0      0.0  0.0   0.0       0.0   0.0    0.0  0.0  0.0  \n",
      "3  ...       0.0    0.0      0.0  0.0   0.0       0.0   0.0    0.0  0.0  0.0  \n",
      "4  ...       0.0    0.0      0.0  0.0   0.0       0.0   0.0    0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 7864 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=299999999, stop_words='english') # You can adjust max_features based on your dataset\n",
    "\n",
    "# Fit the vectorizer on train data and transform it\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['Cleaned_Text'])\n",
    "\n",
    "# Transform the test data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_data['Cleaned_Text'])\n",
    "\n",
    "# Optional: Convert the TF-IDF matrices to DataFrames\n",
    "X_train_tfidf_df = pd.DataFrame(X_train_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "X_test_tfidf_df = pd.DataFrame(X_test_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the TF-IDF features for the train and test data\n",
    "print(\"TF-IDF Features for train data:\\n\", X_train_tfidf_df.head())\n",
    "print(\"TF-IDF Features for test data:\\n\", X_test_tfidf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document vector for first few train documents:\n",
      " 0    [-0.45432404, 0.3931432, 0.23483498, 0.0331704...\n",
      "1    [-0.5345207, 0.4594141, 0.2772847, 0.04183708,...\n",
      "2    [-0.45571983, 0.39207152, 0.23654953, 0.031940...\n",
      "3    [-0.3434233, 0.2919214, 0.18030298, 0.02537513...\n",
      "4    [-0.4250469, 0.35706967, 0.22557741, 0.0309947...\n",
      "Name: Document_Vector, dtype: object\n",
      "Document vector for first few test documents:\n",
      " 0    [-0.50090295, 0.42998785, 0.26453686, 0.040070...\n",
      "1    [-0.49432, 0.4234583, 0.25795934, 0.035648275,...\n",
      "2    [-0.38633806, 0.33103898, 0.19933738, 0.029081...\n",
      "3    [-0.28670678, 0.24181984, 0.15088359, 0.022690...\n",
      "4    [-0.35708034, 0.30365846, 0.18598628, 0.025990...\n",
      "Name: Document_Vector, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Prepare data\n",
    "# Keep sentences as Pandas Series for later use\n",
    "train_sentences = train_data['Cleaned_Text'].apply(lambda x: x.split())\n",
    "test_sentences = test_data['Cleaned_Text'].apply(lambda x: x.split())\n",
    "\n",
    "# Step 2: Train Word2Vec model\n",
    "# Combine train and test sentences for training the model\n",
    "all_sentences = train_sentences.tolist() + test_sentences.tolist()\n",
    "word2vec_model = Word2Vec(sentences=all_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Step 3: Create document vectors\n",
    "def document_vector(doc, model):\n",
    "    # Remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in model.wv]\n",
    "    # If document is not empty, return the average of word vectors\n",
    "    if len(doc) > 0:\n",
    "        return np.mean(model.wv[doc], axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# Step 4: Transform each document in train and test data to vector\n",
    "train_data['Document_Vector'] = train_sentences.apply(lambda x: document_vector(x, word2vec_model))\n",
    "test_data['Document_Vector'] = test_sentences.apply(lambda x: document_vector(x, word2vec_model))\n",
    "\n",
    "# Display a few example vectors\n",
    "print(\"Document vector for first few train documents:\\n\", train_data['Document_Vector'].head())\n",
    "print(\"Document vector for first few test documents:\\n\", test_data['Document_Vector'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL BUILDING\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.995\n",
      "Testing Accuracy: 0.95\n",
      "\n",
      "Classification Report (Test Data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95       502\n",
      "           1       0.92      0.98      0.95       498\n",
      "\n",
      "    accuracy                           0.95      1000\n",
      "   macro avg       0.95      0.95      0.95      1000\n",
      "weighted avg       0.95      0.95      0.95      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Assume 'train_data' and 'test_data' are your DataFrames, and 'Cleaned_Text' contains the processed text\n",
    "# 'Sentiment' is the target column with 1 for positive and 0 for negative sentiment\n",
    "\n",
    "# Step 1: Prepare TF-IDF features\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limit to 5000 features for efficiency\n",
    "X_train_tfidf = vectorizer.fit_transform(train_data['Cleaned_Text'])\n",
    "X_test_tfidf = vectorizer.transform(test_data['Cleaned_Text'])\n",
    "\n",
    "# Step 2: Prepare the target labels\n",
    "y_train = train_data['Label']\n",
    "y_test = test_data['Label']\n",
    "\n",
    "# Step 3: Train Logistic Regression Model\n",
    "model = LogisticRegression(max_iter=500, random_state=42)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Step 4: Make Predictions\n",
    "y_pred_train = model.predict(X_train_tfidf)\n",
    "y_pred_test = model.predict(X_test_tfidf)\n",
    "\n",
    "# Step 5: Evaluate the Model\n",
    "print(\"Training Accuracy:\", accuracy_score(y_train, y_pred_train))\n",
    "print(\"Testing Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"\\nClassification Report (Test Data):\\n\", classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUPPORT VECTOR MACHINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.982\n",
      "\n",
      "Classification Report (Test Data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       502\n",
      "           1       0.98      0.98      0.98       498\n",
      "\n",
      "    accuracy                           0.98      1000\n",
      "   macro avg       0.98      0.98      0.98      1000\n",
      "weighted avg       0.98      0.98      0.98      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.svm import SVC # You may also use LogisticRegression, but we assume you want to keep SVM here \n",
    "from sklearn.metrics import accuracy_score, classification_report \n",
    "import pandas as pd # Assuming 'train_data' and 'test_data' are your DataFrames, and 'Cleaned_Text' contains the processed text # 'Label' is the target column (1 for positive sentiment and 0 for negative sentiment)\n",
    "\n",
    "# Step 2: Prepare the target labels \n",
    "y_train = train_data['Label'] # Assuming 'Label' is your target column for sentiment \n",
    "y_test = test_data['Label']\n",
    "# Step 3: Train the SVM Model\n",
    "svm_model = SVC(kernel='linear', random_state=42) # Linear kernel for SVM \n",
    "svm_model.fit(X_train_tfidf, y_train) \n",
    "# Step 4: Make Predictions \n",
    "y_pred_train = svm_model.predict(X_train_tfidf) \n",
    "y_pred_test = svm_model.predict(X_test_tfidf) \n",
    "# Step 5: Evaluate the Model \n",
    "print(\"Training Accuracy:\", accuracy_score(y_train, y_pred_train))\n",
    "print(\"Testing Accuracy:\", accuracy_score(y_test, y_pred_test)) \n",
    "print(\"\\nClassification Report (Test Data):\\n\", classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAIVE BAYES ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.973\n",
      "Testing Accuracy: 0.827\n",
      "\n",
      "Classification Report (Test Data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.68      0.80       502\n",
      "           1       0.75      0.98      0.85       498\n",
      "\n",
      "    accuracy                           0.83      1000\n",
      "   macro avg       0.86      0.83      0.82      1000\n",
      "weighted avg       0.86      0.83      0.82      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.metrics import accuracy_score, classification_report # Assuming that the data has already been vectorized # X_train_tfidf and X_test_tfidf should already exist as the TF-IDF transformed features # y_train and y_test are your target labels (from 'Label' column)\n",
    "# Step 1: Train the Naive Bayes Model (MultinomialNB)\n",
    "nb_model = MultinomialNB() \n",
    "nb_model.fit(X_train_tfidf, y_train) \n",
    "# Step 2: Make Predictions\n",
    "y_pred_train = nb_model.predict(X_train_tfidf) \n",
    "y_pred_test = nb_model.predict(X_test_tfidf)\n",
    "# Step 3: Evaluate the Model \n",
    "print(\"Training Accuracy:\", accuracy_score(y_train, y_pred_train))\n",
    "print(\"Testing Accuracy:\", accuracy_score(y_test, y_pred_test)) \n",
    "print(\"\\nClassification Report (Test Data):\\n\", classification_report(y_test, y_pred_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
